#!/usr/bin/env python3
"""
Codebase Analyzer - ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹å…¨ä½“ã‚’åˆ†æã—ã€æ”¹å–„ææ¡ˆã‚’ç”Ÿæˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ç”¨æ–¹æ³•:
python codebase_analyzer.py --path /path/to/project --output improvement_tasks.md

ä¸»ãªæ©Ÿèƒ½:
- Python/TypeScript/JavaScriptã‚³ãƒ¼ãƒ‰ã®å“è³ªåˆ†æ (Advanced AST-based analysis for Python)
- ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å€™è£œã®ç‰¹å®š
- è¤‡é›‘åº¦ã®é«˜ã„é–¢æ•°/ãƒ¡ã‚½ãƒƒãƒ‰ã®æ¤œå‡º
- é‡è¤‡ã‚³ãƒ¼ãƒ‰ã®ç™ºè¦‹ (AST hash-based sophisticated detection)
- ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•é¡Œã®ã‚¹ã‚­ãƒ£ãƒ³
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ– (Parallel processing support)
"""

import argparse
import ast
import builtins
import contextlib
import hashlib
import json
import os
import re
import subprocess
import sys
import time
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path

# Python AST analysis imports - Other languages can be added as needed
try:
    import rope.base.project

    ROPE_AVAILABLE = True
except ImportError:
    ROPE_AVAILABLE = False
    print("Warning: rope library not available. Advanced Python analysis disabled.")

# jscpd for JavaScript/TypeScript duplication detection
JSCPD_AVAILABLE = False
try:
    # Check if jscpd is available
    result = subprocess.run(
        ["jscpd", "--version"], capture_output=True, text=True, timeout=10
    )
    if result.returncode == 0:
        JSCPD_AVAILABLE = True
        print(f"âœ… jscpd available: {result.stdout.strip()}")
    else:
        print("Warning: jscpd not found in PATH")
except (subprocess.TimeoutExpired, FileNotFoundError):
    print("Warning: jscpd not available. Install with: npm install -g jscpd")
    print("JavaScript/TypeScript duplication detection will use basic regex analysis")


@dataclass
class CodeIssue:
    """ã‚³ãƒ¼ãƒ‰ã®å•é¡Œç‚¹ã‚’è¡¨ç¾ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""

    file_path: str
    line_number: int
    issue_type: str  # 'complexity', 'duplication', 'security', 'testing', 'performance'
    severity: str  # 'high', 'medium', 'low'
    title: str
    description: str
    suggestion: str
    effort_estimate: str  # '15min', '1h', '4h', '1d'


@dataclass
class FunctionInfo:
    """é–¢æ•°ã®è©³ç´°æƒ…å ±"""

    name: str
    file: str
    lineno: int
    args: list[str]
    body_hash: str
    body_lines: int
    complexity: int = 0


@dataclass
class ClassInfo:
    """ã‚¯ãƒ©ã‚¹ã®è©³ç´°æƒ…å ±"""

    name: str
    file: str
    lineno: int
    methods: list[str]
    body_hash: str
    body_lines: int
    complexity: int = 0


@dataclass
class FileMetrics:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""

    file_path: str
    lines_of_code: int
    functions: int
    classes: int
    max_complexity: int
    test_coverage: float | None = None
    language: str = "unknown"


class JSCPDConfig:
    """jscpdã®è¨­å®šã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        # JavaScript/TypeScriptæ‹¡å¼µå­
        self.js_ts_extensions = {".js", ".jsx", ".ts", ".tsx"}

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ç„¡è¦–ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.default_ignore_patterns = [
            "**/node_modules/**",
            "**/dist/**",
            "**/build/**",
            "**/coverage/**",
            "**/.git/**",
            "**/.next/**",
            "**/.nuxt/**",
            "**/vendor/**",
        ]

        # è¨­å®šå€¤
        self.threshold = 0  # 0%ä»¥ä¸Šã®é‡è¤‡ã‚’æ¤œå‡º
        self.timeout_seconds = 300  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        self.min_lines_for_high_severity = 50
        self.min_lines_for_medium_severity = 20
        self.temp_file_prefix = ".jscpd_files"
        self.temp_file_suffix = ".txt"

        # é‡è¤‡é‡è¦åº¦ã®å·¥æ•°è¦‹ç©ã‚‚ã‚Š
        self.effort_estimates = {"high": "4h", "medium": "2h", "low": "1h"}


class JSCPDAnalyzer:
    """jscpdã‚’ä½¿ç”¨ã—ãŸJavaScript/TypeScripté‡è¤‡ã‚³ãƒ¼ãƒ‰åˆ†æã‚¯ãƒ©ã‚¹"""

    def __init__(self, project_path: str, config: JSCPDConfig = None):
        self.project_path = Path(project_path).resolve()
        self.config = config or JSCPDConfig()
        self.temp_files: list[Path] = []  # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ç”¨ã®è¿½è·¡ãƒªã‚¹ãƒˆ

    def run_jscpd_analysis(self, files: list[Path]) -> list[dict]:
        """jscpdã‚’å®Ÿè¡Œã—ã¦é‡è¤‡ã‚³ãƒ¼ãƒ‰ã‚’æ¤œå‡º"""
        if not JSCPD_AVAILABLE:
            return []

        # JavaScript/TypeScriptãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        js_ts_files = [f for f in files if f.suffix in self.config.js_ts_extensions]
        if not js_ts_files:
            return []

        try:
            # jscpdã‚³ãƒãƒ³ãƒ‰ã‚’æ§‹ç¯‰
            cmd = self._build_jscpd_command(js_ts_files, files)

            # jscpdã‚’å®Ÿè¡Œ
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=self.config.timeout_seconds,
                cwd=str(self.project_path),
            )

            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self._cleanup_temp_files()

            if result.returncode != 0:
                error_msg = result.stderr.strip() if result.stderr else "Unknown error"
                print(f"jscpd failed (exit code {result.returncode}): {error_msg}")
                return []

            # JSONçµæœã‚’è§£æ
            try:
                jscpd_result = json.loads(result.stdout)
                return self._parse_jscpd_results(jscpd_result, js_ts_files)
            except json.JSONDecodeError as e:
                # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ã‚¨ãƒ©ãƒ¼å‡ºåŠ›ã®ä¸€éƒ¨ã‚’è¡¨ç¤º
                stderr_preview = result.stderr[:200] if result.stderr else "No stderr"
                print(f"Failed to parse jscpd JSON output: {e}")
                print(f"Stderr preview: {stderr_preview}")
                stdout_preview = result.stdout[:200] if result.stdout else "No stdout"
                print(f"Stdout preview: {stdout_preview}")
                return []

        except subprocess.TimeoutExpired:
            print(
                f"jscpd analysis timed out after {self.config.timeout_seconds} seconds"
            )
            self._cleanup_temp_files()
            return []
        except Exception as e:
            print(f"jscpd analysis failed: {e}")
            self._cleanup_temp_files()
            return []

    def _build_jscpd_command(
        self, js_ts_files: list[Path], all_files: list[Path]
    ) -> list[str]:
        """jscpdã‚³ãƒãƒ³ãƒ‰ã‚’æ§‹ç¯‰"""
        cmd = [
            "jscpd",
            str(self.project_path),
            "--format",
            "json",
            "--output",
            "-",
            "--threshold",
            str(self.config.threshold),
        ]

        # ç„¡è¦–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ 
        for pattern in self.config.default_ignore_patterns:
            cmd.extend(["--ignore", pattern])

        # ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å¯¾è±¡ã«ã™ã‚‹å ´åˆ
        if len(js_ts_files) < len(all_files):
            try:
                file_list_file = self._create_temp_file_list(js_ts_files)
                cmd.extend(["--files-list", str(file_list_file)])
            except Exception as e:
                print(f"Warning: Could not create file list for jscpd: {e}")

        return cmd

    def _create_temp_file_list(self, js_ts_files: list[Path]) -> Path:
        """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’ä½œæˆ"""
        import uuid

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        temp_filename = f"{self.config.temp_file_prefix}_{uuid.uuid4().hex[:8]}{self.config.temp_file_suffix}"
        temp_file = self.project_path / temp_filename

        try:
            with open(temp_file, "w", encoding="utf-8") as f:
                for js_file in js_ts_files:
                    try:
                        rel_path = js_file.relative_to(self.project_path)
                        f.write(f"{rel_path}\n")
                    except ValueError:
                        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå¤–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯çµ¶å¯¾ãƒ‘ã‚¹ã‚’ä½¿ç”¨
                        f.write(f"{js_file}\n")

            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒªã‚¹ãƒˆã«è¿½åŠ 
            self.temp_files.append(temp_file)
            return temp_file

        except Exception as e:
            # ä½œæˆå¤±æ•—æ™‚ã¯ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’è©¦ã¿ã‚‹
            with contextlib.suppress(builtins.BaseException):
                temp_file.unlink(missing_ok=True)
            raise e

    def _cleanup_temp_files(self):
        """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        for temp_file in self.temp_files:
            try:
                temp_file.unlink(missing_ok=True)
            except Exception as e:
                print(f"Warning: Could not delete temp file {temp_file}: {e}")
        self.temp_files.clear()

    def _parse_jscpd_results(
        self, jscpd_result: dict, target_files: list[Path]
    ) -> list[dict]:
        """jscpdã®çµæœã‚’è§£æã—ã¦CodeIssueå½¢å¼ã«å¤‰æ›"""
        duplications = []

        # jscpdã®çµæœå½¢å¼ã‚’æ¤œè¨¼
        if not isinstance(jscpd_result, dict):
            print("Warning: jscpd result is not a dictionary")
            return duplications

        # ç•°ãªã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹çµæœå½¢å¼ã«å¯¾å¿œ
        duplications_key = (
            "duplication" if "duplication" in jscpd_result else "duplications"
        )
        if duplications_key not in jscpd_result:
            print("No duplications found in jscpd result")
            return duplications

        duplication_list = jscpd_result[duplications_key]
        if not isinstance(duplication_list, list):
            print(
                f"Warning: Expected list for '{duplications_key}', got {type(duplication_list)}"
            )
            return duplications

        for i, dup in enumerate(duplication_list):
            try:
                if not isinstance(dup, dict):
                    print(f"Warning: Duplication {i} is not a dictionary")
                    continue

                if "fragments" not in dup or not isinstance(dup["fragments"], list):
                    continue

                fragments = dup["fragments"]
                if len(fragments) < 2:
                    continue

                first_fragment = self._validate_fragment(fragments[0], i, 0)
                if not first_fragment:
                    continue

                # é‡è¤‡ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†
                for j, fragment in enumerate(fragments[1:], 1):
                    duplication_info = self._process_fragment(
                        fragment, first_fragment, dup, target_files, i, j
                    )
                    if duplication_info:
                        duplications.append(duplication_info)

            except Exception as e:
                print(f"Error parsing jscpd duplication {i}: {e}")
                continue

        return duplications

    def _validate_fragment(
        self, fragment: dict, dup_index: int, frag_index: int
    ) -> dict | None:
        """ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œè¨¼"""
        if not isinstance(fragment, dict):
            print(
                f"Warning: Fragment {frag_index} in duplication {dup_index} is not a dictionary"
            )
            return None

        file_path = fragment.get("file", "")
        if not file_path:
            print(
                f"Warning: Fragment {frag_index} in duplication {dup_index} has no file path"
            )
            return None

        return fragment

    def _process_fragment(
        self,
        fragment: dict,
        first_fragment: dict,
        dup: dict,
        target_files: list[Path],
        dup_index: int,
        frag_index: int,
    ) -> dict | None:
        """å€‹åˆ¥ã®é‡è¤‡ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†"""
        try:
            file_path = fragment.get("file", "")
            if not file_path:
                return None

            # ãƒ‘ã‚¹ã‚’æ­£è¦åŒ–
            try:
                abs_file_path = (self.project_path / file_path).resolve()
            except (ValueError, OSError) as e:
                print(f"Warning: Invalid file path '{file_path}': {e}")
                return None

            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
            if not self._is_target_file(abs_file_path, target_files):
                return None

            # é‡è¤‡ã®é‡è¦åº¦ã‚’åˆ¤å®š
            lines_count = fragment.get("size", 0)
            severity = self._determine_severity(lines_count)
            effort = self.config.effort_estimates[severity]

            # ç›¸å¯¾ãƒ‘ã‚¹ã‚’è¨ˆç®—
            try:
                rel_file_path = abs_file_path.relative_to(self.project_path)
            except ValueError:
                # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå¤–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯çµ¶å¯¾ãƒ‘ã‚¹ã‚’ä½¿ç”¨
                rel_file_path = abs_file_path

            return {
                "file_path": str(rel_file_path),
                "line_number": max(1, fragment.get("start", 0) + 1),
                "end_line": max(1, fragment.get("start", 0) + lines_count),
                "lines_count": max(0, lines_count),
                "similarity": max(0.0, min(100.0, dup.get("similarity", 0))),
                "first_occurrence": {
                    "file": first_fragment.get("file", "unknown"),
                    "line": max(1, first_fragment.get("start", 0) + 1),
                },
                "severity": severity,
                "effort_estimate": effort,
                "duplication_id": f"{dup_index}_{frag_index}",
            }

        except Exception as e:
            print(f"Error processing fragment {frag_index}: {e}")
            return None

    def _is_target_file(self, abs_file_path: Path, target_files: list[Path]) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª"""
        try:
            return any(
                abs_file_path.samefile(target_file.resolve())
                for target_file in target_files
            )
        except (ValueError, OSError):
            return False

    def _determine_severity(self, lines_count: int) -> str:
        """é‡è¤‡ã®é‡è¦åº¦ã‚’åˆ¤å®š"""
        if lines_count >= self.config.min_lines_for_high_severity:
            return "high"
        if lines_count >= self.config.min_lines_for_medium_severity:
            return "medium"
        return "low"


class AnalyzerConfig:
    """ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼å…¨ä½“ã®è¨­å®šã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        # åˆ†æå¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­
        self.target_extensions = {
            ".ts",
            ".tsx",
            ".js",
            ".jsx",
            ".vue",
            ".py",
            ".java",
            ".cpp",
            ".c",
            ".cs",
            ".php",
            ".rb",
            ".go",
            ".rs",
            ".swift",
            ".kt",
        }

        # ç„¡è¦–ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.ignore_dirs = {
            "node_modules",
            ".git",
            "dist",
            "build",
            "coverage",
            ".next",
            ".nuxt",
            "__pycache__",
            ".pytest_cache",
            "vendor",
            ".vscode",
            ".idea",
        }

        # ä¸¦åˆ—å‡¦ç†ã®ã—ãã„å€¤
        self.parallel_processing_threshold = 10
        self.default_max_workers = 4

        # è¤‡é›‘åº¦ã®ã—ãã„å€¤
        self.high_complexity_threshold = 20
        self.medium_complexity_threshold = 10

        # é–¢æ•°ã®è¡Œæ•°ã—ãã„å€¤
        self.long_function_threshold = 50
        self.very_long_function_threshold = 100


class AdvancedCodeAnalyzer:
    """é«˜åº¦ãªã‚³ãƒ¼ãƒ‰åˆ†æã‚¯ãƒ©ã‚¹ (AST-based for Python + jscpd for JS/TS)"""

    def __init__(
        self,
        project_path: str,
        config: AnalyzerConfig = None,
        jscpd_config: JSCPDConfig = None,
    ):
        self.project_path = Path(project_path).resolve()
        self.config = config or AnalyzerConfig()
        self.jscpd_config = jscpd_config or JSCPDConfig()

        # åˆ†æçµæœã®æ ¼ç´
        self.functions: list[FunctionInfo] = []
        self.classes: list[ClassInfo] = []
        self.current_file = None

        # Pythonç”¨ropeãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ (ropeãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆ)
        self.rope_project = None
        if ROPE_AVAILABLE:
            try:
                self.rope_project = rope.base.project.Project(str(self.project_path))
            except Exception as e:
                print(f"Warning: Could not initialize rope project: {e}")

        # jscpdã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼
        self.jscpd_analyzer = JSCPDAnalyzer(str(self.project_path), self.jscpd_config)

    def __del__(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†"""
        if hasattr(self, "rope_project") and self.rope_project:
            with contextlib.suppress(builtins.BaseException):
                self.rope_project.close()

    def set_file(self, file_path: str) -> None:
        """ç¾åœ¨åˆ†æä¸­ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¨­å®š"""
        self.current_file = file_path

    def analyze_python_file(self, file_path: Path) -> dict:
        """Pythonãƒ•ã‚¡ã‚¤ãƒ«ã‚’é«˜åº¦ã«åˆ†æ (rope + AST)"""
        # Future: Add similar implementations for other languages (JavaScript/TypeScript, etc.)
        if not ROPE_AVAILABLE or file_path.suffix != ".py":
            return self._basic_analysis(file_path)

        try:
            # ropeã«ã‚ˆã‚‹åˆ†æ
            if self.rope_project:
                return self._rope_analysis(file_path)
            return self._ast_analysis(file_path)
        except Exception as e:
            print(f"Warning: Advanced analysis failed for {file_path}: {e}")
            return self._basic_analysis(file_path)

    def _ast_analysis(self, file_path: Path) -> dict:
        """ASTãƒ™ãƒ¼ã‚¹ã®åˆ†æå®Ÿè£…"""
        self.set_file(str(file_path))

        try:
            with open(file_path, encoding="utf-8") as f:
                source_code = f.read()

            tree = ast.parse(source_code)
            self._walk_ast(tree)

            return {
                "functions": self.functions.copy(),
                "classes": self.classes.copy(),
                "success": True,
            }
        except Exception as e:
            print(f"AST analysis failed for {file_path}: {e}")
            return {"functions": [], "classes": [], "success": False}

    def _rope_analysis(self, file_path: Path) -> dict:
        """ropeã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªåˆ†æ"""
        try:
            # ropeã§ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å–å¾—
            resource = self.rope_project.get_resource(
                str(file_path.relative_to(self.project_path))
            )
            if not resource:
                return self._ast_analysis(file_path)

            module = self.rope_project.get_pymodule(resource)
            if not module:
                return self._ast_analysis(file_path)

            # ASTåˆ†æã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._ast_analysis(file_path)
        except Exception:
            return self._ast_analysis(file_path)

    def _basic_analysis(self, file_path: Path) -> dict:
        """åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆåˆ†æ (å…¨è¨€èªå…±é€š)"""
        try:
            with open(file_path, encoding="utf-8") as f:
                content = f.read()

            # å„è¨€èªã«ç‰¹åŒ–ã—ãŸãƒ‘ãƒ¼ã‚µãƒ¼ã‚’å®Ÿè£…
            if file_path.suffix in {".ts", ".tsx", ".js", ".jsx"}:
                # jscpdåˆ©ç”¨å¯èƒ½ã®å ´åˆã¯åŸºæœ¬è§£æã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                # jscpdã®é‡è¤‡æ¤œå‡ºã¯åˆ¥é€”å®Ÿè¡Œ
                return self._js_basic_analysis(content, str(file_path))
            if file_path.suffix == ".vue":
                # Note: Vue SFC parser could leverage jscpd for script block duplication detection
                return self._vue_basic_analysis(content, str(file_path))
            if file_path.suffix == ".py":
                return self._ast_analysis(file_path)
            # Future: Implement specialized parsers for other languages
            return self._generic_analysis(content, str(file_path))

        except Exception as e:
            print(f"Basic analysis failed for {file_path}: {e}")
            return {"functions": [], "classes": [], "success": False}

    def _generic_analysis(self, content: str, file_path: str) -> dict:
        """æ±ç”¨çš„ãªã‚³ãƒ¼ãƒ‰åˆ†æ (è¨€èªä¸æ˜ã®å ´åˆ)"""
        functions = []
        classes = []

        # æ±ç”¨çš„ãªé–¢æ•°ãƒ»ã‚¯ãƒ©ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œå‡º
        func_patterns = [
            r"function\s+(\w+)\s*\(",
            r"def\s+(\w+)\s*\(",
            r"(\w+)\s*\([^)]*\)\s*\{",
            r"public\s+\w+\s+(\w+)\s*\(",
            r"private\s+\w+\s+(\w+)\s*\(",
        ]

        class_patterns = [
            r"class\s+(\w+)",
            r"interface\s+(\w+)",
            r"type\s+(\w+)",
            r"struct\s+(\w+)",
            r"enum\s+(\w+)",
        ]

        for pattern in func_patterns:
            for match in re.finditer(pattern, content):
                func_info = FunctionInfo(
                    name=match.group(1),
                    file=file_path,
                    lineno=content[: match.start()].count("\n") + 1,
                    args=[],
                    body_hash=hashlib.md5(match.group(0).encode()).hexdigest(),
                    body_lines=match.group(0).count("\n") + 1,
                )
                functions.append(func_info)

        for pattern in class_patterns:
            for match in re.finditer(pattern, content):
                class_info = ClassInfo(
                    name=match.group(1),
                    file=file_path,
                    lineno=content[: match.start()].count("\n") + 1,
                    methods=[],
                    body_hash=hashlib.md5(match.group(0).encode()).hexdigest(),
                    body_lines=match.group(0).count("\n") + 1,
                )
                classes.append(class_info)

        return {"functions": functions, "classes": classes, "success": True}

    def _js_basic_analysis(self, content: str, file_path: str) -> dict:
        """JavaScript/TypeScriptã®åŸºæœ¬çš„ãªæ­£è¦è¡¨ç¾ãƒ™ãƒ¼ã‚¹åˆ†æ"""
        # Future: Consider using @babel/parser or TypeScript compiler API for AST parsing
        functions = []
        classes = []

        # é–¢æ•°ã®æŠ½å‡º (æ­£è¦è¡¨ç¾ãƒ™ãƒ¼ã‚¹ã®æš«å®šå®Ÿè£…)
        func_patterns = [
            r"(?:export\s+)?(?:async\s+)?function\s+(\w+)\s*\([^)]*\)",
            r"const\s+(\w+)\s*=\s*(?:async\s*)?(?:\([^)]*\)\s*)?=>",
            r"(\w+)\s*:\s*(?:async\s*)?(?:\([^)]*\)\s*)?=>",
            r"(?:async\s+)?(\w+)\s*\([^)]*\)\s*\{",
        ]

        for pattern in func_patterns:
            for match in re.finditer(pattern, content):
                # Skip comments (simple check)
                line_start = content.rfind("\n", 0, match.start()) + 1
                line = content[line_start : match.end()]
                if line.strip().startswith("//") or line.strip().startswith("*"):
                    continue

                func_info = FunctionInfo(
                    name=match.group(1),
                    file=file_path,
                    lineno=content[: match.start()].count("\n") + 1,
                    args=[],  # Placeholder: Argument extraction could be improved with AST parsing
                    body_hash=hashlib.md5(match.group(0).encode()).hexdigest(),
                    body_lines=match.group(0).count("\n") + 1,
                )
                functions.append(func_info)

        # ã‚¯ãƒ©ã‚¹ã®æŠ½å‡º (å®šç¾©ã®ã¿ã‚’å¯¾è±¡ã¨ã—ã€re-exportã‚’é™¤å¤–)
        class_patterns = [
            r"(?:class|interface)\s+(\w+)[^{]*\{",  # class/interface definition start
            r"type\s+(\w+)\s*=",  # type alias definition
        ]

        for pattern in class_patterns:
            for match in re.finditer(pattern, content):
                # Skip comments
                line_start = content.rfind("\n", 0, match.start()) + 1
                line = content[line_start : match.end()]
                if line.strip().startswith("//") or line.strip().startswith("*"):
                    continue

                class_info = ClassInfo(
                    name=match.group(1),
                    file=file_path,
                    lineno=content[: match.start()].count("\n") + 1,
                    methods=[],  # Placeholder: Method extraction could be improved with AST parsing
                    body_hash=hashlib.md5(match.group(0).encode()).hexdigest(),
                    body_lines=match.group(0).count("\n") + 1,
                )
                classes.append(class_info)

        return {"functions": functions, "classes": classes, "success": True}

    def _vue_basic_analysis(self, content: str, file_path: str) -> dict:
        """Vue SFCãƒ•ã‚¡ã‚¤ãƒ«ã®åŸºæœ¬çš„ãªåˆ†æ - scriptãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰é–¢æ•°ã¨ã‚¯ãƒ©ã‚¹ã‚’æŠ½å‡º"""
        functions = []
        classes = []

        try:
            # Vue SFCã‹ã‚‰<script>ãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
            script_patterns = [
                r"<script[^>]*>(.*?)</script>",
                r"<script[^>]*setup[^>]*>(.*?)</script>",
            ]

            script_content = ""
            for pattern in script_patterns:
                matches = re.finditer(pattern, content, re.DOTALL | re.IGNORECASE)
                for match in matches:
                    script_content += match.group(1) + "\n"

            if script_content.strip():
                # JavaScript/TypeScriptã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§é–¢æ•°ã¨ã‚¯ãƒ©ã‚¹ã‚’æŠ½å‡º
                func_patterns = [
                    r"(?:export\s+)?(?:async\s+)?function\s+(\w+)\s*\([^)]*\)",
                    r"const\s+(\w+)\s*=\s*(?:async\s*)?(?:\([^)]*\)\s*)?=>",
                    r"(\w+)\s*:\s*(?:async\s*)?(?:\([^)]*\)\s*)?=>",
                    r"(?:async\s+)?(\w+)\s*\([^)]*\)\s*\{",
                ]

                # é–¢æ•°ã®æŠ½å‡º
                for pattern in func_patterns:
                    for match in re.finditer(pattern, script_content):
                        # ã‚³ãƒ¡ãƒ³ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—
                        line_start = script_content.rfind("\n", 0, match.start()) + 1
                        line = script_content[line_start : match.end()]
                        if line.strip().startswith("//") or line.strip().startswith("*"):
                            continue

                        # Vueã®SFCã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã®è¡Œç•ªå·ã‚’è¨ˆç®—
                        script_block_start = content.find(match.group(0))
                        if script_block_start != -1:
                            line_no = content[: script_block_start].count("\n") + 1
                        else:
                            line_no = script_content[: match.start()].count("\n") + 1

                        func_info = FunctionInfo(
                            name=match.group(1),
                            file=file_path,
                            lineno=line_no,
                            args=[],  # å¼•æ•°æŠ½å‡ºã®æ”¹å–„ã¯å°†æ¥å®Ÿè£…
                            body_hash="",  # Vue SFCç”¨ã®ç°¡æ˜“ãƒãƒƒã‚·ãƒ¥
                            body_lines=1,  # ç°¡æ˜“çš„ãªè¡Œæ•°ã‚«ã‚¦ãƒ³ãƒˆ
                            complexity=1,
                        )
                        functions.append(func_info)

                # ã‚¯ãƒ©ã‚¹ã®æŠ½å‡º
                class_pattern = r"(?:export\s+)?(?:default\s+)?class\s+(\w+)"
                for match in re.finditer(class_pattern, script_content):
                    line_start = script_content.rfind("\n", 0, match.start()) + 1
                    line = script_content[line_start : match.end()]
                    if line.strip().startswith("//") or line.strip().startswith("*"):
                        continue

                    script_block_start = content.find(match.group(0))
                    if script_block_start != -1:
                        line_no = content[: script_block_start].count("\n") + 1
                    else:
                        line_no = script_content[: match.start()].count("\n") + 1

                    class_info = ClassInfo(
                        name=match.group(1),
                        file=file_path,
                        lineno=line_no,
                        methods=[],  # ãƒ¡ã‚½ãƒƒãƒ‰æŠ½å‡ºã®æ”¹å–„ã¯å°†æ¥å®Ÿè£…
                        body_hash="",  # Vue SFCç”¨ã®ç°¡æ˜“ãƒãƒƒã‚·ãƒ¥
                        body_lines=1,  # ç°¡æ˜“çš„ãªè¡Œæ•°ã‚«ã‚¦ãƒ³ãƒˆ
                        complexity=1,
                    )
                    classes.append(class_info)

                return {"functions": functions, "classes": classes, "success": True}
            else:
                # <script>ãƒ–ãƒ­ãƒƒã‚¯ãŒãªã„å ´åˆ
                return {"functions": [], "classes": [], "success": True}

        except Exception as e:
            print(f"Warning: Vue SFC analysis failed for {file_path}: {e}")
            return {"functions": [], "classes": [], "success": False}

    def _walk_ast(self, node):
        """ASTã‚’ã‚¦ã‚©ãƒ¼ã‚¯ã—ã¦é–¢æ•°ã¨ã‚¯ãƒ©ã‚¹ã‚’æŠ½å‡º"""
        for child in ast.walk(node):
            if isinstance(child, ast.FunctionDef | ast.AsyncFunctionDef):
                self._extract_function(child)
            elif isinstance(child, ast.ClassDef):
                self._extract_class(child)

    def _extract_function(self, node):
        """é–¢æ•°æƒ…å ±ã‚’æŠ½å‡º"""
        func_info = FunctionInfo(
            name=node.name,
            file=self.current_file,
            lineno=node.lineno,
            args=[arg.arg for arg in node.args.args],
            body_hash=self._get_node_hash(node),
            body_lines=node.end_lineno - node.lineno + 1
            if hasattr(node, "end_lineno")
            else 0,
            complexity=self._calculate_ast_complexity(node),
        )
        self.functions.append(func_info)

    def _extract_class(self, node):
        """ã‚¯ãƒ©ã‚¹æƒ…å ±ã‚’æŠ½å‡º"""
        methods = []
        for item in node.body:
            if isinstance(item, ast.FunctionDef | ast.AsyncFunctionDef):
                methods.append(item.name)

        class_info = ClassInfo(
            name=node.name,
            file=self.current_file,
            lineno=node.lineno,
            methods=methods,
            body_hash=self._get_node_hash(node),
            body_lines=node.end_lineno - node.lineno + 1
            if hasattr(node, "end_lineno")
            else 0,
            complexity=self._calculate_ast_complexity(node),
        )
        self.classes.append(class_info)

    def _get_node_hash(self, node) -> str:
        """ASTãƒãƒ¼ãƒ‰ã®ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ"""
        node_str = ast.dump(node, include_attributes=False)
        return hashlib.md5(node_str.encode()).hexdigest()

    def _calculate_ast_complexity(self, node) -> int:
        """ASTãƒãƒ¼ãƒ‰ã®å¾ªç’°çš„è¤‡é›‘åº¦ã‚’è¨ˆç®—"""
        complexity = 1  # åŸºæœ¬è¤‡é›‘åº¦

        for child in ast.walk(node):
            if isinstance(
                child, ast.If | ast.While | ast.For | ast.AsyncFor | ast.ExceptHandler
            ):
                complexity += 1
            elif isinstance(child, ast.BoolOp):
                complexity += len(child.values) - 1
            elif isinstance(child, ast.With, ast.AsyncWith):
                complexity += 1

        return complexity


class CodebaseAnalyzer:
    """ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹åˆ†æã‚¯ãƒ©ã‚¹"""

    def __init__(
        self,
        project_path: str,
        config: AnalyzerConfig = None,
        jscpd_config: JSCPDConfig = None,
    ):
        self.project_path = Path(project_path).resolve()
        self.config = config or AnalyzerConfig()
        self.jscpd_config = jscpd_config or JSCPDConfig()

        # åˆ†æçµæœã®æ ¼ç´
        self.issues: list[CodeIssue] = []
        self.file_metrics: list[FileMetrics] = []
        self.timing_info: dict = {}

        # é«˜åº¦ãªã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ã‚’åˆæœŸåŒ–
        self.advanced_analyzer = AdvancedCodeAnalyzer(
            str(self.project_path), self.config, self.jscpd_config
        )

    def analyze_project(self, max_workers: int = 4) -> dict:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã‚’åˆ†æ (ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ç‰ˆ)"""
        start_time = time.time()

        print(f"ğŸ” Analyzing project: {self.project_path}")
        if ROPE_AVAILABLE:
            print("ğŸ Using advanced Python AST analysis (rope)")

        # Step 1: ãƒ•ã‚¡ã‚¤ãƒ«åé›†
        step1_start = time.time()
        files = self._collect_files()
        self.timing_info["file_collection"] = {
            "time": time.time() - step1_start,
            "description": "File discovery and filtering",
        }
        print(f"ğŸ“ Found {len(files)} files to analyze")

        # Step 2: é«˜åº¦ãªãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ (ä¸¦åˆ—å‡¦ç†)
        step2_start = time.time()
        all_functions: list[FunctionInfo] = []
        all_classes: list[ClassInfo] = []

        if len(files) > self.config.parallel_processing_threshold:
            print(f"ğŸš€ Using parallel processing with {max_workers} workers")
            all_functions, all_classes = self._analyze_files_parallel(
                files, max_workers
            )
        else:
            print("ğŸ”§ Using sequential processing (small dataset)")
            for file_path in files:
                print(f"  Analyzing: {file_path.relative_to(self.project_path)}")
                result = self.advanced_analyzer.analyze_python_file(file_path)
                if result["success"]:
                    all_functions.extend(result["functions"])
                    all_classes.extend(result["classes"])
                self._analyze_file_basic(file_path)

        self.timing_info["file_analysis"] = {
            "time": time.time() - step2_start,
            "workers": max_workers
            if len(files) > self.config.parallel_processing_threshold
            else 1,
            "description": "Advanced AST and basic analysis",
        }

        # Step 3: é«˜åº¦ãªé‡è¤‡ã‚³ãƒ¼ãƒ‰æ¤œå‡º
        step3_start = time.time()
        self._detect_advanced_duplicates(all_functions, all_classes, files)
        self.timing_info["duplicate_detection"] = {
            "time": time.time() - step3_start,
            "description": "AST hash-based (Python) + jscpd (JS/TS) duplication detection",
        }

        # Step 4: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®å•é¡Œåˆ†æ
        step4_start = time.time()
        self._analyze_project_issues()
        self.timing_info["project_analysis"] = {
            "time": time.time() - step4_start,
            "description": "Project-wide issue analysis",
        }

        # çµæœã‚’é›†è¨ˆ
        total_time = time.time() - start_time
        self.timing_info["total"] = {
            "time": total_time,
            "description": "Complete analysis pipeline",
            "files_processed": len(files),
            "functions_found": len(all_functions),
            "classes_found": len(all_classes),
        }

        return self._generate_report()

    def _analyze_files_parallel(
        self, files: list[Path], max_workers: int
    ) -> tuple[list[FunctionInfo], list[ClassInfo]]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸¦åˆ—å‡¦ç†ã§åˆ†æ"""
        all_functions = []
        all_classes = []

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # åˆ†æã‚¿ã‚¹ã‚¯ã‚’æŠ•å…¥
            future_to_file = {
                executor.submit(
                    self._analyze_single_file_advanced, file_path
                ): file_path
                for file_path in files
            }

            # çµæœã‚’åé›†
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    result = future.result()
                    if result["success"]:
                        all_functions.extend(result["functions"])
                        all_classes.extend(result["classes"])

                    # ALWAYS run basic analysis to populate file_metrics and detect other issues
                    self._analyze_file_basic(file_path)

                    print(f"  âœ“ Analyzed: {file_path.relative_to(self.project_path)}")
                except Exception as e:
                    print(
                        f"    âš ï¸  Error analyzing {file_path.relative_to(self.project_path)}: {e}"
                    )
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¦åŸºæœ¬åˆ†æã‚’å®Ÿè¡Œ
                    self._analyze_file_basic(file_path)

        return all_functions, all_classes

    def _analyze_single_file_advanced(self, file_path: Path) -> dict:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®é«˜åº¦ãªåˆ†æ (ä¸¦åˆ—å‡¦ç†ç”¨)"""
        try:
            # æ–°ã—ã„Analyzerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã®ãŸã‚ï¼‰
            analyzer = AdvancedCodeAnalyzer(str(self.project_path))
            return analyzer.analyze_python_file(file_path)
        except Exception as e:
            print(f"Error in advanced analysis for {file_path}: {e}")
            return {"functions": [], "classes": [], "success": False}

    def _collect_files(self) -> list[Path]:
        """åˆ†æå¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†"""
        files = []
        for root, dirs, filenames in os.walk(self.project_path):
            # ç„¡è¦–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’é™¤å¤–
            dirs[:] = [d for d in dirs if d not in self.config.ignore_dirs]

            for filename in filenames:
                file_path = Path(root) / filename
                if file_path.suffix in self.config.target_extensions:
                    files.append(file_path)

        return files

    def _analyze_file_basic(self, file_path: Path):
        """å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®åŸºæœ¬åˆ†æ (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨)"""
        try:
            with open(file_path, encoding="utf-8") as f:
                content = f.read()

            lines = content.split("\n")
            metrics = self._calculate_metrics(file_path, content, lines)
            self.file_metrics.append(metrics)

            # è¤‡é›‘åº¦ã®é«˜ã„é–¢æ•°ã‚’æ¤œå‡º
            self._detect_complex_functions(file_path, content, lines)

            # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•é¡Œã‚’ã‚¹ã‚­ãƒ£ãƒ³
            self._scan_security_issues(file_path, content, lines)

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œã‚’æ¤œå‡º
            self._detect_performance_issues(file_path, content, lines)

            # ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’åˆ†æ
            if "test" in file_path.name or "spec" in file_path.name:
                self._analyze_test_coverage(file_path, content)

        except Exception as e:
            print(f"    âš ï¸  Error analyzing {file_path}: {e}")

    def _detect_advanced_duplicates(
        self,
        all_functions: list[FunctionInfo],
        all_classes: list[ClassInfo],
        files: list[Path],
    ):
        """é«˜åº¦ãªé‡è¤‡ã‚³ãƒ¼ãƒ‰æ¤œå‡º (ASTãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ + jscpd)"""
        # Pythonã®é‡è¤‡æ¤œå‡º (ASTãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹)
        self._detect_duplicate_functions(all_functions)
        self._detect_duplicate_classes(all_classes)
        self._detect_similar_function_names(all_functions)

        # JavaScript/TypeScriptã®é‡è¤‡æ¤œå‡º (jscpd)
        if JSCPD_AVAILABLE:
            print("ğŸ” Running jscpd for JavaScript/TypeScript duplication detection...")
            jscpd_duplications = (
                self.advanced_analyzer.jscpd_analyzer.run_jscpd_analysis(files)
            )
            self._process_jscpd_results(jscpd_duplications)
        else:
            # jscpdãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯åŸºæœ¬ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®é‡è¤‡æ¤œå‡ºã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            print(
                "âš ï¸ jscpd not available, using basic text-based duplication detection for JS/TS"
            )
            self._detect_js_ts_basic_duplicates(files)

    def _detect_duplicate_functions(self, functions: list[FunctionInfo]):
        """é‡è¤‡é–¢æ•°ã‚’ASTãƒãƒƒã‚·ãƒ¥ã§æ¤œå‡º"""
        hash_groups = defaultdict(list)

        # ASTãƒãƒƒã‚·ãƒ¥ã§é–¢æ•°ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        for func in functions:
            hash_groups[func.body_hash].append(func)

        # é‡è¤‡ã‚’å ±å‘Š
        for _hash_val, funcs in hash_groups.items():
            if len(funcs) > 1:
                for func in funcs[1:]:  # 2ç•ªç›®ä»¥é™ã‚’å ±å‘Š
                    self.issues.append(
                        CodeIssue(
                            file_path=func.file,
                            line_number=func.lineno,
                            issue_type="duplication",
                            severity="medium",
                            title=f"é‡è¤‡é–¢æ•°: {func.name}",
                            description=f"ã“ã®é–¢æ•°ã¯{funcs[0].file}ã®é–¢æ•°ã¨åŒä¸€å®Ÿè£…ã§ã™ã€‚",
                            suggestion="å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã¨ã—ã¦æŠ½å‡ºã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚",
                            effort_estimate="1h",
                        )
                    )

    def _detect_duplicate_classes(self, classes: list[ClassInfo]):
        """é‡è¤‡ã‚¯ãƒ©ã‚¹ã‚’ASTãƒãƒƒã‚·ãƒ¥ã§æ¤œå‡º"""
        hash_groups = defaultdict(list)

        # ASTãƒãƒƒã‚·ãƒ¥ã§ã‚¯ãƒ©ã‚¹ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        for cls in classes:
            hash_groups[cls.body_hash].append(cls)

        # é‡è¤‡ã‚’å ±å‘Š
        for _hash_val, classes_group in hash_groups.items():
            if len(classes_group) > 1:
                for cls in classes_group[1:]:  # 2ç•ªç›®ä»¥é™ã‚’å ±å‘Š
                    self.issues.append(
                        CodeIssue(
                            file_path=cls.file,
                            line_number=cls.lineno,
                            issue_type="duplication",
                            severity="high",
                            title=f"é‡è¤‡ã‚¯ãƒ©ã‚¹: {cls.name}",
                            description=f"ã“ã®ã‚¯ãƒ©ã‚¹ã¯{classes_group[0].file}ã®ã‚¯ãƒ©ã‚¹ã¨åŒä¸€å®Ÿè£…ã§ã™ã€‚",
                            suggestion="åŸºåº•ã‚¯ãƒ©ã‚¹ã¨ã—ã¦æŠ½å‡ºã™ã‚‹ã‹ã€ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚",
                            effort_estimate="4h",
                        )
                    )

    def _detect_similar_function_names(self, functions: list[FunctionInfo]):
        """é¡ä¼¼åå‰ã®é–¢æ•°ã‚’æ¤œå‡º"""
        name_groups = defaultdict(list)

        # é–¢æ•°åã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        for func in functions:
            name_groups[func.name].append(func)

        # åŒåã§ç•°ãªã‚‹å®Ÿè£…ã®é–¢æ•°ã‚’å ±å‘Š
        for _name, funcs in name_groups.items():
            if len(funcs) > 1:
                # ç•°ãªã‚‹å®Ÿè£…ã‹ãƒã‚§ãƒƒã‚¯
                hashes = {func.body_hash for func in funcs}
                if len(hashes) > 1:
                    for func in funcs[1:]:  # 2ç•ªç›®ä»¥é™ã‚’å ±å‘Š
                        self.issues.append(
                            CodeIssue(
                                file_path=func.file,
                                line_number=func.lineno,
                                issue_type="duplication",
                                severity="medium",
                                title=f"é¡ä¼¼é–¢æ•°å: {func.name}",
                                description="åŒåé–¢æ•°ãŒè¤‡æ•°å­˜åœ¨ã—ã€å®Ÿè£…ãŒç•°ãªã‚Šã¾ã™ã€‚",
                                suggestion="é–¢æ•°åã‚’ã‚ˆã‚Šå…·ä½“çš„ã«ã™ã‚‹ã‹ã€å®Ÿè£…ã‚’çµ±ä¸€ã—ã¦ãã ã•ã„ã€‚",
                                effort_estimate="30min",
                            )
                        )

    def _process_jscpd_results(self, jscpd_duplications: list[dict]):
        """jscpdã®é‡è¤‡æ¤œå‡ºçµæœã‚’å‡¦ç†ã—ã¦Issueã«å¤‰æ›"""
        if not jscpd_duplications:
            print("ğŸ“Š No duplications found by jscpd")
            return

        print(f"ğŸ“Š Found {len(jscpd_duplications)} duplications via jscpd")

        for dup in jscpd_duplications:
            try:
                first_occurrence = dup.get("first_occurrence", {})
                description = (
                    f"ã“ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¯ {first_occurrence.get('file', 'unknown')}:"
                    f"{first_occurrence.get('line', '?')} ã¨ {dup['similarity']:.1f}% é¡ä¼¼ã—ã¦ã„ã¾ã™ã€‚"
                    f"({dup['lines_count']} è¡Œ)"
                )

                suggestion = (
                    "å…±é€šã®é–¢æ•°ã¾ãŸã¯ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã¨ã—ã¦æŠ½å‡ºã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
                    "ã¾ãŸã¯ã€é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ã«ã‚³ãƒ¼ãƒ‰ã®è¨­è¨ˆã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚"
                )

                self.issues.append(
                    CodeIssue(
                        file_path=dup["file_path"],
                        line_number=dup["line_number"],
                        issue_type="duplication",
                        severity=dup["severity"],
                        title=f'jscpdæ¤œå‡ºé‡è¤‡ã‚³ãƒ¼ãƒ‰ ({dup["lines_count"]}è¡Œ)',
                        description=description,
                        suggestion=suggestion,
                        effort_estimate=dup["effort_estimate"],
                    )
                )
            except KeyError as e:
                print(f"Warning: Missing key in jscpd result: {e}")
                continue

    def _detect_js_ts_basic_duplicates(self, files: list[Path]):
        """jscpdãŒåˆ©ç”¨ã§ããªã„å ´åˆã®åŸºæœ¬çš„ãªJS/TSé‡è¤‡æ¤œå‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        [f for f in files if f.suffix in {".js", ".jsx", ".ts", ".tsx"}]

        # å…ƒã®é‡è¤‡æ¤œå‡ºãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        print("âš ï¸ Using basic text-based duplication detection for JS/TS files")
        # ã“ã®å®Ÿè£…ã¯æ—¢å­˜ã®_detect_duplicatesãƒ¡ã‚½ãƒƒãƒ‰ã‚’æµç”¨ã§ãã‚‹
        self._detect_duplicates()

    def _calculate_metrics(
        self, file_path: Path, content: str, lines: list[str]
    ) -> FileMetrics:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—"""
        loc = len([line for line in lines if line.strip()])

        # è¨€èªã‚’æ¤œå‡º
        language = self._detect_language(file_path)

        # é–¢æ•°ã¨ã‚¯ãƒ©ã‚¹ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆç°¡æ˜“çš„ãªå®Ÿè£…ï¼‰
        if file_path.suffix in {".ts", ".tsx", ".js", ".jsx"}:
            # Current: Regex-based counting - could be improved with TypeScript/JavaScript AST parser
            functions = len(
                re.findall(
                    r"(?:function\s+\w+|const\s+\w+\s*=\s*(?:\([^)]*\)\s*)?=>|\w+\s*:\s*\([^)]*\)\s*=>)",
                    content,
                )
            )
            classes = len(
                re.findall(r"(?:class\s+\w+|interface\s+\w+|type\s+\w+)", content)
            )
        elif file_path.suffix == ".py":
            # Current: Regex-based counting - could use AST parser results for improved accuracy
            functions = len(re.findall(r"def\s+\w+", content))
            classes = len(re.findall(r"class\s+\w+", content))
        elif file_path.suffix == ".vue":
            # Future: Implement Vue SFC parser for better analysis
            functions = 0
            classes = 0
        else:
            # Future: Add support for other languages
            functions = 0
            classes = 0

        max_complexity = self._estimate_complexity(content)

        return FileMetrics(
            file_path=str(file_path.relative_to(self.project_path)),
            lines_of_code=loc,
            functions=functions,
            classes=classes,
            max_complexity=max_complexity,
            language=language,
        )

    def _detect_language(self, file_path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨€èªã‚’æ¤œå‡º"""
        extension_map = {
            ".py": "python",
            ".js": "javascript",
            ".jsx": "javascript",
            ".ts": "typescript",
            ".tsx": "typescript",
            ".vue": "vue",
            ".java": "java",
            ".cpp": "cpp",
            ".c": "c",
            ".cs": "csharp",
            ".php": "php",
            ".rb": "ruby",
            ".go": "go",
            ".rs": "rust",
            ".swift": "swift",
            ".kt": "kotlin",
            # More languages can be added as needed
        }

        return extension_map.get(file_path.suffix, "unknown")

    def _estimate_complexity(self, content: str) -> int:
        """ã‚³ãƒ¼ãƒ‰ã®è¤‡é›‘åº¦ã‚’ç°¡æ˜“çš„ã«æ¨å®š"""
        # åˆ¶å¾¡æ§‹é€ ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        complexity_keywords = [
            "if",
            "else",
            "for",
            "while",
            "do",
            "switch",
            "case",
            "try",
            "catch",
            "&&",
            "||",
            "?",
            "??",
            "break",
            "continue",
            "return",
        ]

        complexity = 1  # åŸºæœ¬è¤‡é›‘åº¦
        for keyword in complexity_keywords:
            complexity += len(re.findall(r"\b" + re.escape(keyword) + r"\b", content))

        return min(complexity, 50)  # ä¸Šé™ã‚’è¨­å®š

    def _detect_complex_functions(
        self, file_path: Path, content: str, lines: list[str]
    ):
        """è¤‡é›‘åº¦ã®é«˜ã„é–¢æ•°ã‚’æ¤œå‡º"""
        # é•·ã„é–¢æ•°ã‚’æ¤œå‡ºï¼ˆ50è¡Œè¶…éï¼‰
        functions = re.finditer(
            r"(?:function\s+\w+|const\s+\w+\s*=\s*(?:\([^)]*\)\s*)?=>|\w+\s*:\s*\([^)]*\)\s*=>|def\s+\w+)[^{]*\{((?:[^{}]*\{[^{}]*\})*[^{}]*)",
            content,
            re.MULTILINE,
        )

        for match in functions:
            func_content = match.group(0)
            func_lines = func_content.count("\n")

            if func_lines > self.config.long_function_threshold:
                start_line = content[: match.start()].count("\n") + 1
                severity = (
                    "high"
                    if func_lines > self.config.very_long_function_threshold
                    else "medium"
                )
                effort = (
                    "4h"
                    if func_lines > self.config.very_long_function_threshold
                    else "2h"
                )

                self.issues.append(
                    CodeIssue(
                        file_path=str(file_path.relative_to(self.project_path)),
                        line_number=start_line,
                        issue_type="complexity",
                        severity=severity,
                        title="é–¢æ•°ãŒé•·ã™ãã¾ã™",
                        description=f"ã“ã®é–¢æ•°ã¯{func_lines}è¡Œã‚ã‚Šã€æ¨å¥¨ã•ã‚Œã‚‹{self.config.long_function_threshold}è¡Œã‚’å¤§å¹…ã«è¶…ãˆã¦ã„ã¾ã™ã€‚",
                        suggestion="é–¢æ•°ã‚’è¤‡æ•°ã®å°ã•ãªé–¢æ•°ã«åˆ†å‰²ã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚",
                        effort_estimate=effort,
                    )
                )

    def _scan_security_issues(self, file_path: Path, content: str, lines: list[str]):
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•é¡Œã‚’ã‚¹ã‚­ãƒ£ãƒ³"""
        # Skip security scan for analyzer scripts to avoid self-detection of patterns
        if file_path.name in ["codebase_analyzer.py", "code_review.py"]:
            return

        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã©ã†ã‹åˆ¤å®š
        is_test_file = (
            "test" in str(file_path).lower() or "spec" in str(file_path).lower()
        )

        security_patterns = [
            (r"eval\s*\(", "eval()ã®ä½¿ç”¨ã¯ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯ãŒã‚ã‚Šã¾ã™", "high", "4h"),
            (
                r"innerHTML\s*=",
                "innerHTMLã®ç›´æ¥ä»£å…¥ã¯XSSãƒªã‚¹ã‚¯ãŒã‚ã‚Šã¾ã™",
                "high",
                "2h",
            ),
            (
                r"document\.write\s*\(",
                "document.write()ã¯ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯ãŒã‚ã‚Šã¾ã™",
                "high",
                "2h",
            ),
            (
                r"console\.log\s*\(",
                "æœ¬ç•ªã‚³ãƒ¼ãƒ‰ã«console.logãŒå«ã¾ã‚Œã¦ã„ã¾ã™",
                "low",
                "15min",
            ),
            # Ignore mock/test values
            (
                r'password\s*[:=]\s*["\'](?!(?:mock|test|placeholder|dummy|env|process))[^"\']+["\']',
                "ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã™",
                "high",
                "1h",
            ),
            (
                r'api[_-]?key\s*[:=]\s*["\'](?!(?:mock|test|placeholder|dummy|env|process))[^"\']+["\']',
                "ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸAPIã‚­ãƒ¼ãŒã‚ã‚Šã¾ã™",
                "high",
                "1h",
            ),
        ]

        for i, line in enumerate(lines, 1):
            for pattern, description, severity, effort in security_patterns:
                # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã§ã®ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰/ã‚­ãƒ¼ã¯ç„¡è¦–
                if is_test_file and ("password" in pattern or "api" in pattern):
                    continue

                if re.search(pattern, line, re.IGNORECASE):
                    self.issues.append(
                        CodeIssue(
                            file_path=str(file_path.relative_to(self.project_path)),
                            line_number=i,
                            issue_type="security",
                            severity=severity,
                            title="ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ä¸Šã®æ‡¸å¿µ",
                            description=description,
                            suggestion="ã‚ˆã‚Šå®‰å…¨ãªä»£æ›¿æ–¹æ³•ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚",
                            effort_estimate=effort,
                        )
                    )

    def _detect_performance_issues(
        self, file_path: Path, content: str, lines: list[str]
    ):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œã‚’æ¤œå‡º"""
        performance_patterns = [
            (
                r"for\s*\([^)]*\bin\b[^)]+\)",
                "for...inãƒ«ãƒ¼ãƒ—ã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒä½ã„å ´åˆãŒã‚ã‚Šã¾ã™",
                "medium",
                "30min",
            ),
            (
                r"Array\.prototype\.forEach\.call",
                "Array.forEach.callã¯æœ€é©ã§ã¯ã‚ã‚Šã¾ã›ã‚“",
                "low",
                "15min",
            ),
            (
                r'setTimeout\s*\(\s*["\']',
                "setTimeoutã«æ–‡å­—åˆ—ã‚’ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„",
                "medium",
                "30min",
            ),
        ]

        for i, line in enumerate(lines, 1):
            for pattern, description, severity, effort in performance_patterns:
                if re.search(pattern, line):
                    self.issues.append(
                        CodeIssue(
                            file_path=str(file_path.relative_to(self.project_path)),
                            line_number=i,
                            issue_type="performance",
                            severity=severity,
                            title="ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æ”¹å–„å¯èƒ½æ€§",
                            description=description,
                            suggestion="ã‚ˆã‚ŠåŠ¹ç‡çš„ãªå®Ÿè£…æ–¹æ³•ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚",
                            effort_estimate=effort,
                        )
                    )

    def _detect_duplicates(self):
        """é‡è¤‡ã‚³ãƒ¼ãƒ‰ã‚’æ¤œå‡ºï¼ˆç°¡æ˜“çš„ãªå®Ÿè£…ï¼‰"""
        code_blocks = {}

        BLOCK_SIZE = 10
        MIN_CHARS = 100

        for metrics in self.file_metrics:
            file_path = self.project_path / metrics.file_path
            try:
                with open(file_path, encoding="utf-8") as f:
                    content = f.read()

                lines = content.split("\n")
                if len(lines) < BLOCK_SIZE:
                    continue

                for i in range(len(lines) - (BLOCK_SIZE - 1)):
                    window_lines = lines[i : i + BLOCK_SIZE]

                    # Skip if any line is an import/export/package statement (reduce noise)
                    if any(
                        l.strip().startswith(
                            (
                                "import ",
                                "export ",
                                "from ",
                                "package ",
                                "using ",
                                "include ",
                            )
                        )
                        for l in window_lines
                    ):
                        continue

                    # Skip if mostly comments
                    if (
                        sum(
                            1
                            for l in window_lines
                            if l.strip().startswith(("/", "*", "#"))
                        )
                        > BLOCK_SIZE / 2
                    ):
                        continue

                    block = "\n".join(window_lines).strip()

                    if len(block) > MIN_CHARS:
                        if block in code_blocks:
                            code_blocks[block].append((metrics.file_path, i + 1))
                        else:
                            code_blocks[block] = [(metrics.file_path, i + 1)]
            except (SyntaxError, ValueError, UnicodeDecodeError):
                continue

        # é‡è¤‡ãƒ–ãƒ­ãƒƒã‚¯ã‚’å ±å‘Š
        for block, locations in code_blocks.items():
            if len(locations) > 1:
                for file_path, line_num in locations[1:]:  # 2ç•ªç›®ä»¥é™ã®å ´æ‰€ã‚’å ±å‘Š
                    self.issues.append(
                        CodeIssue(
                            file_path=file_path,
                            line_number=line_num,
                            issue_type="duplication",
                            severity="medium",
                            title="é‡è¤‡ã‚³ãƒ¼ãƒ‰ã®å¯èƒ½æ€§",
                            description=f"ã“ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆ{BLOCK_SIZE}è¡Œï¼‰ã¯ä»–ã®å ´æ‰€ã§ã‚‚è¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚",
                            suggestion="å…±é€šé–¢æ•°ã¨ã—ã¦æŠ½å‡ºã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚",
                            effort_estimate="1h",
                        )
                    )

    def _analyze_project_issues(self):
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®å•é¡Œã‚’åˆ†æ"""
        total_files = len(self.file_metrics)
        sum(m.lines_of_code for m in self.file_metrics)

        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®æ¯”ç‡
        test_files = len(
            [
                m
                for m in self.file_metrics
                if "test" in m.file_path or "spec" in m.file_path
            ]
        )
        test_ratio = test_files / total_files if total_files > 0 else 0

        if test_ratio < 0.1:  # 10%æœªæº€
            self.issues.append(
                CodeIssue(
                    file_path="ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“",
                    line_number=0,
                    issue_type="testing",
                    severity="high",
                    title="ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ãŒä¸è¶³ã—ã¦ã„ã¾ã™",
                    description=f"ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®æ¯”ç‡: {test_ratio:.1%} (æ¨å¥¨: 20%+)",
                    suggestion="ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã€çµ±åˆãƒ†ã‚¹ãƒˆã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚",
                    effort_estimate="2d",
                )
            )

        # å¹³å‡è¤‡é›‘åº¦
        avg_complexity = (
            sum(m.max_complexity for m in self.file_metrics) / total_files
            if total_files > 0
            else 0
        )
        if avg_complexity > self.config.high_complexity_threshold:
            self.issues.append(
                CodeIssue(
                    file_path="ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“",
                    line_number=0,
                    issue_type="complexity",
                    severity="medium",
                    title="å¹³å‡è¤‡é›‘åº¦ãŒé«˜ã„ã§ã™",
                    description=f"å¹³å‡è¤‡é›‘åº¦: {avg_complexity:.1f} (æ¨å¥¨: {self.config.medium_complexity_threshold}ä»¥ä¸‹)",
                    suggestion="ã‚³ãƒ¼ãƒ‰ã®å˜ç´”åŒ–ã€é–¢æ•°åˆ†å‰²ãªã©ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚",
                    effort_estimate="1d",
                )
            )

    def _analyze_test_coverage(self, file_path: Path, content: str):
        """ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’åˆ†æ"""
        # ç°¡æ˜“çš„ãªãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ
        test_patterns = [
            r"describe\s*\(",
            r"it\s*\(",
            r"test\s*\(",
            r"expect\s*\(",
        ]

        coverage_score = 0
        for pattern in test_patterns:
            coverage_score += len(re.findall(pattern, content))

        # å¯¾å¿œã™ã‚‹ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        source_file = None
        for metrics in self.file_metrics:
            if metrics.file_path.replace("test/", "").replace("spec/", "").replace(
                ".test.", "."
            ).replace(".spec.", ".") == str(
                file_path.relative_to(self.project_path)
            ).replace("test/", "").replace("spec/", "").replace(".test.", ".").replace(
                ".spec.", "."
            ):
                source_file = metrics
                break

        if source_file and coverage_score < 5:  # ãƒ†ã‚¹ãƒˆãŒå°‘ãªã„å ´åˆ
            self.issues.append(
                CodeIssue(
                    file_path=str(file_path.relative_to(self.project_path)),
                    line_number=0,
                    issue_type="testing",
                    severity="medium",
                    title="ãƒ†ã‚¹ãƒˆãŒä¸ååˆ†ã§ã™",
                    description=f"ã“ã®ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ãŒå°‘ãªã„ã§ã™ (ã‚¹ã‚³ã‚¢: {coverage_score})",
                    suggestion="æ›´å¤šã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚",
                    effort_estimate="1h",
                )
            )

    def _generate_report(self) -> dict:
        """åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ (ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ä»˜ã)"""
        # é‡è¦åº¦ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        issues_by_severity = {"high": [], "medium": [], "low": []}
        for issue in self.issues:
            issues_by_severity[issue.severity].append(issue)

        # ã‚¿ã‚¤ãƒ—ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        issues_by_type = {}
        for issue in self.issues:
            if issue.issue_type not in issues_by_type:
                issues_by_type[issue.issue_type] = []
            issues_by_type[issue.issue_type].append(issue)

        # è¨€èªåˆ¥çµ±è¨ˆ
        language_stats = {}
        for metrics in self.file_metrics:
            lang = metrics.language
            if lang not in language_stats:
                language_stats[lang] = {
                    "files": 0,
                    "lines": 0,
                    "functions": 0,
                    "classes": 0,
                    "avg_complexity": 0,
                }
            stats = language_stats[lang]
            stats["files"] += 1
            stats["lines"] += metrics.lines_of_code
            stats["functions"] += metrics.functions
            stats["classes"] += metrics.classes

        # å¹³å‡è¤‡é›‘åº¦ã‚’è¨ˆç®—
        for lang, stats in language_stats.items():
            lang_files = [m for m in self.file_metrics if m.language == lang]
            if lang_files:
                stats["avg_complexity"] = sum(
                    m.max_complexity for m in lang_files
                ) / len(lang_files)

        return {
            "summary": {
                "total_files": len(self.file_metrics),
                "total_lines": sum(m.lines_of_code for m in self.file_metrics),
                "total_issues": len(self.issues),
                "high_priority_issues": len(issues_by_severity["high"]),
                "medium_priority_issues": len(issues_by_severity["medium"]),
                "low_priority_issues": len(issues_by_severity["low"]),
                "languages_supported": len(language_stats),
                "rope_available": ROPE_AVAILABLE,
                "jscpd_available": JSCPD_AVAILABLE,
            },
            "issues_by_severity": issues_by_severity,
            "issues_by_type": issues_by_type,
            "file_metrics": self.file_metrics,
            "detailed_issues": self.issues,
            "language_statistics": language_stats,
            "timing_info": self.timing_info,
            "recommendations": self._generate_recommendations(),
            "performance_summary": self._generate_performance_summary(),
        }

    def _generate_performance_summary(self) -> dict:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        if not self.timing_info:
            return {}

        total_time = self.timing_info.get("total", {}).get("time", 0)
        total_files = self.timing_info.get("total", {}).get("files_processed", 0)

        return {
            "total_analysis_time": f"{total_time:.2f}s",
            "files_per_second": f"{total_files / total_time:.1f}"
            if total_time > 0
            else "N/A",
            "timing_breakdown": {
                step: info["time"]
                for step, info in self.timing_info.items()
                if step != "total"
            },
            "bottleneck": max(
                [
                    (step, info["time"])
                    for step, info in self.timing_info.items()
                    if step != "total"
                ],
                key=lambda x: x[1],
                default=("N/A", 0),
            )[0],
        }

    def _generate_recommendations(self) -> list[str]:
        """æ”¹å–„æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []

        high_count = len([i for i in self.issues if i.severity == "high"])
        if high_count > 0:
            recommendations.append(
                f"ğŸ”´ å„ªå…ˆåº¦ã®é«˜ã„å•é¡ŒãŒ{high_count}ä»¶ã‚ã‚Šã¾ã™ã€‚ã¾ãšã“ã‚Œã‚‰ã®å¯¾å‡¦ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
            )

        # æœ€ã‚‚å¤šã„å•é¡Œã‚¿ã‚¤ãƒ—
        type_counts = {}
        for issue in self.issues:
            type_counts[issue.issue_type] = type_counts.get(issue.issue_type, 0) + 1

        if type_counts:
            most_common = max(type_counts.items(), key=lambda x: x[1])
            recommendations.append(
                f"ğŸ“Š {most_common[0]}é–¢é€£ã®å•é¡ŒãŒ{most_common[1]}ä»¶ã¨æœ€ã‚‚å¤šã„ã§ã™ã€‚"
            )

        # å·¥æ•°è¦‹ç©ã‚‚ã‚Š
        total_effort = sum(
            1
            if issue.effort_estimate == "15min"
            else 4
            if issue.effort_estimate == "1h"
            else 32
            if issue.effort_estimate == "4h"
            else 40
            if issue.effort_estimate == "1d"
            else 0
            for issue in self.issues
        )

        if total_effort > 0:
            recommendations.append(
                f"â±ï¸ å…¨å•é¡Œã®ä¿®æ­£ã«ã¯ç´„{total_effort/8:.1f}æ—¥ã‚’è¦‹ç©ã‚‚ã‚Šã¾ã™ã€‚"
            )

        return recommendations


def generate_markdown_report(analysis_result: dict, output_path: str):
    """Markdownå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ (æ©Ÿèƒ½å¼·åŒ–ç‰ˆ)"""
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(
            f"""# ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹æ”¹å–„ææ¡ˆãƒ¬ãƒãƒ¼ãƒˆ

ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š ã‚µãƒãƒªãƒ¼

- **åˆ†æãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {analysis_result['summary']['total_files']} ãƒ•ã‚¡ã‚¤ãƒ«
- **ç·ã‚³ãƒ¼ãƒ‰è¡Œæ•°**: {analysis_result['summary']['total_lines']:,} è¡Œ
- **ç™ºè¦‹ã•ã‚ŒãŸå•é¡Œ**: {analysis_result['summary']['total_issues']} ä»¶
  - ğŸ”´ é«˜å„ªå…ˆåº¦: {analysis_result['summary']['high_priority_issues']} ä»¶
  - ğŸŸ¡ ä¸­å„ªå…ˆåº¦: {analysis_result['summary']['medium_priority_issues']} ä»¶
  - ğŸŸ¢ ä½å„ªå…ˆåº¦: {analysis_result['summary']['low_priority_issues']} ä»¶
- **ã‚µãƒãƒ¼ãƒˆè¨€èª**: {analysis_result['summary']['languages_supported']} ç¨®é¡
- **é«˜åº¦ãªPythonåˆ†æ**: {'æœ‰åŠ¹' if analysis_result['summary']['rope_available'] else 'ç„¡åŠ¹'}
- **jscpd (JS/TSé‡è¤‡æ¤œå‡º)**: {'æœ‰åŠ¹' if analysis_result['summary']['jscpd_available'] else 'ç„¡åŠ¹'}

"""
        )

        # æ¨å¥¨äº‹é …
        if analysis_result["recommendations"]:
            f.write("## ğŸ’¡ æ¨å¥¨äº‹é …\n\n")
            for rec in analysis_result["recommendations"]:
                f.write(f"- {rec}\n")
            f.write("\n")

        # å„ªå…ˆåº¦åˆ¥ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ
        f.write("## ğŸ¯ å„ªå…ˆåº¦åˆ¥ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ\n\n")

        for severity in ["high", "medium", "low"]:
            issues = analysis_result["issues_by_severity"][severity]
            if not issues:
                continue

            severity_emoji = {"high": "ğŸ”´", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}
            severity_text = {
                "high": "é«˜å„ªå…ˆåº¦",
                "medium": "ä¸­å„ªå…ˆåº¦",
                "low": "ä½å„ªå…ˆåº¦",
            }

            f.write(
                f"### {severity_emoji[severity]} {severity_text[severity]} ({len(issues)}ä»¶)\n\n"
            )

            for i, issue in enumerate(issues, 1):
                f.write(f"#### {i}. {issue.title}\n\n")
                f.write(f"**ãƒ•ã‚¡ã‚¤ãƒ«**: `{issue.file_path}:{issue.line_number}`\n\n")
                f.write(f"**èª¬æ˜**: {issue.description}\n\n")
                f.write(f"**ææ¡ˆ**: {issue.suggestion}\n\n")
                f.write(f"**è¦‹ç©ã‚‚ã‚Š**: {issue.effort_estimate}\n\n")
                f.write("---\n\n")

        # å•é¡Œã‚¿ã‚¤ãƒ—åˆ¥åˆ†æ
        f.write("## ğŸ“ˆ å•é¡Œã‚¿ã‚¤ãƒ—åˆ¥åˆ†æ\n\n")

        for issue_type, issues in analysis_result["issues_by_type"].items():
            if not issues:
                continue

            type_names = {
                "complexity": "è¤‡é›‘åº¦",
                "duplication": "é‡è¤‡ã‚³ãƒ¼ãƒ‰",
                "security": "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£",
                "performance": "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
                "testing": "ãƒ†ã‚¹ãƒˆ",
            }

            type_name = type_names.get(issue_type, issue_type)
            f.write(f"### {type_name} ({len(issues)}ä»¶)\n\n")

            # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥é›†è¨ˆ
            file_counts = {}
            for issue in issues:
                if issue.file_path not in file_counts:
                    file_counts[issue.file_path] = 0
                file_counts[issue.file_path] += 1

            f.write("**å•é¡Œã®å¤šã„ãƒ•ã‚¡ã‚¤ãƒ«**:\n")
            for file_path, count in sorted(
                file_counts.items(), key=lambda x: x[1], reverse=True
            )[:5]:
                f.write(f"- `{file_path}`: {count}ä»¶\n")
            f.write("\n")

        # è¨€èªåˆ¥çµ±è¨ˆ
        if "language_statistics" in analysis_result:
            f.write("## ğŸŒ è¨€èªåˆ¥çµ±è¨ˆ\n\n")
            f.write(
                "| è¨€èª | ãƒ•ã‚¡ã‚¤ãƒ«æ•° | ã‚³ãƒ¼ãƒ‰è¡Œæ•° | é–¢æ•°æ•° | ã‚¯ãƒ©ã‚¹æ•° | å¹³å‡è¤‡é›‘åº¦ |\n"
            )
            f.write("|------|----------|----------|--------|--------|------------|\n")

            for lang, stats in sorted(
                analysis_result["language_statistics"].items(),
                key=lambda x: x[1]["lines"],
                reverse=True,
            ):
                f.write(
                    f"| {lang} | {stats['files']} | {stats['lines']:,} | {stats['functions']} | {stats['classes']} | {stats['avg_complexity']:.1f} |\n"
                )

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼
        if (
            "performance_summary" in analysis_result
            and analysis_result["performance_summary"]
        ):
            perf = analysis_result["performance_summary"]
            f.write("## âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼\n\n")
            f.write(f"- **ç·åˆ†ææ™‚é–“**: {perf['total_analysis_time']}\n")
            f.write(f"- **å‡¦ç†é€Ÿåº¦**: {perf['files_per_second']} ãƒ•ã‚¡ã‚¤ãƒ«/ç§’\n")
            f.write(f"- **ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**: {perf['bottleneck']}\n\n")

            f.write("### å‡¦ç†æ™‚é–“ã®å†…è¨³\n\n")
            f.write("| ã‚¹ãƒ†ãƒƒãƒ— | æ™‚é–“ (ç§’) | èª¬æ˜ |\n")
            f.write("|--------|-----------|------|\n")

            for step, time_taken in perf["timing_breakdown"].items():
                # ã‚¿ã‚¤ãƒŸãƒ³ã‚°æƒ…å ±ã‹ã‚‰èª¬æ˜ã‚’å–å¾—
                desc = (
                    analysis_result.get("timing_info", {})
                    .get(step, {})
                    .get("description", step)
                )
                f.write(f"| {step} | {time_taken:.3f} | {desc} |\n")
            f.write("\n")

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        f.write("## ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹\n\n")
        f.write("| ãƒ•ã‚¡ã‚¤ãƒ« | è¨€èª | ã‚³ãƒ¼ãƒ‰è¡Œæ•° | é–¢æ•°æ•° | ã‚¯ãƒ©ã‚¹æ•° | æœ€å¤§è¤‡é›‘åº¦ |\n")
        f.write("|--------|------|----------|--------|--------|------------|\n")

        # è¤‡é›‘åº¦é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_metrics = sorted(
            analysis_result["file_metrics"],
            key=lambda x: x.max_complexity,
            reverse=True,
        )

        for metrics in sorted_metrics[:20]:  # ä¸Šä½20ä»¶ã‚’è¡¨ç¤º
            f.write(
                f"| `{metrics.file_path}` | {metrics.language} | {metrics.lines_of_code:,} | {metrics.functions} | {metrics.classes} | {metrics.max_complexity} |\n"
            )

        if len(sorted_metrics) > 20:
            f.write(f"| ... ãªã© {len(sorted_metrics)-20}ãƒ•ã‚¡ã‚¤ãƒ« | | | | | |\n")

    print(f"ğŸ“ ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã‚’åˆ†æã—ã¦æ”¹å–„ææ¡ˆã‚’ç”Ÿæˆ (æ©Ÿèƒ½å¼·åŒ–ç‰ˆ)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä¾‹:
  python codebase_analyzer.py --path /path/to/project
  python codebase_analyzer.py --path . --workers 8 --verbose --format json
  python codebase_analyzer.py --path . --output custom_report.md

æ³¨æ„äº‹é …:
  - jscpdã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯: npm install -g jscpd
  - ropeã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯: pip install rope
        """,
    )
    parser.add_argument(
        "--path", required=True, help="åˆ†æã™ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‘ã‚¹ (å¿…é ˆ)"
    )
    parser.add_argument(
        "--output",
        default="improvement_tasks.md",
        help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: improvement_tasks.md)",
    )
    parser.add_argument(
        "--format",
        choices=["markdown", "json"],
        default="markdown",
        help="å‡ºåŠ›å½¢å¼ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: markdown)",
    )
    parser.add_argument(
        "--workers",
        type=int,
        default=None,
        help="ä¸¦åˆ—å‡¦ç†ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: CPUã‚³ã‚¢æ•°ã¾ãŸã¯4)",
    )
    parser.add_argument("--verbose", action="store_true", help="è©³ç´°ãªåˆ†ææƒ…å ±ã‚’è¡¨ç¤º")
    parser.add_argument(
        "--timeout",
        type=int,
        default=300,
        help="jscpdã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 300)",
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=0,
        help="é‡è¤‡æ¤œå‡ºã®ã—ãã„å€¤ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0)",
    )

    args = parser.parse_args()

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å­˜åœ¨ç¢ºèªã¨ãƒ‘ã‚¹ã®æ¤œè¨¼
    try:
        project_path = Path(args.path).resolve()
        if not project_path.exists():
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {project_path}")
            sys.exit(1)
        if not project_path.is_dir():
            print(
                f"âŒ ã‚¨ãƒ©ãƒ¼: æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã¯ã‚ã‚Šã¾ã›ã‚“: {project_path}"
            )
            sys.exit(1)
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ‘ã‚¹ã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        sys.exit(1)

    if args.verbose:
        print("ğŸ”§ è¨­å®š:")
        print(f"  - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹: {args.path}")
        print(f"  - å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {args.output}")
        print(f"  - å‡ºåŠ›å½¢å¼: {args.format}")
        print(f"  - ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {args.workers}")
        print(
            f"  - ropeãƒ©ã‚¤ãƒ–ãƒ©ãƒª (Python): {'åˆ©ç”¨å¯èƒ½' if ROPE_AVAILABLE else 'åˆ©ç”¨ä¸å¯'}"
        )
        print(f"  - jscpd (JS/TS): {'åˆ©ç”¨å¯èƒ½' if JSCPD_AVAILABLE else 'åˆ©ç”¨ä¸å¯'}")
        if not JSCPD_AVAILABLE:
            print("    ğŸ’¡ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•: npm install -g jscpd")
        print()

    # è¨­å®šã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–
    config = AnalyzerConfig()
    jscpd_config = JSCPDConfig()

    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§è¨­å®šã‚’ä¸Šæ›¸ã
    if args.timeout is not None:
        jscpd_config.timeout_seconds = args.timeout
    if args.threshold is not None:
        jscpd_config.threshold = args.threshold

    # ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’æ±ºå®š
    import multiprocessing

    default_workers = min(multiprocessing.cpu_count(), 8)
    max_workers = args.workers or config.default_max_workers or default_workers

    if args.verbose:
        print(f"  - ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {max_workers}")
        print(f"  - jscpdã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {jscpd_config.timeout_seconds}ç§’")
        print(f"  - é‡è¤‡æ¤œå‡ºã—ãã„å€¤: {jscpd_config.threshold}%")
        print()

    # åˆ†æå®Ÿè¡Œ
    start_time = time.time()
    analyzer = CodebaseAnalyzer(str(project_path), config, jscpd_config)
    result = analyzer.analyze_project(max_workers=max_workers)
    analysis_time = time.time() - start_time

    # çµæœã‚’å‡ºåŠ›
    if args.format == "json":
        with open(args.output, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2, default=str)
    else:
        generate_markdown_report(result, args.output)

    # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    total_issues = result["summary"]["total_issues"]
    high_issues = result["summary"]["high_priority_issues"]
    medium_issues = result["summary"]["medium_priority_issues"]
    low_issues = result["summary"]["low_priority_issues"]

    print(f"\nğŸ‰ åˆ†æå®Œäº†! æ™‚é–“: {analysis_time:.2f}ç§’")
    print(f"ğŸ“‹ ç™ºè¦‹ã•ã‚ŒãŸå•é¡Œ: {total_issues}ä»¶")
    print(f"  ğŸ”´ é«˜å„ªå…ˆåº¦: {high_issues}ä»¶")
    print(f"  ğŸŸ¡ ä¸­å„ªå…ˆåº¦: {medium_issues}ä»¶")
    print(f"  ğŸŸ¢ ä½å„ªå…ˆåº¦: {low_issues}ä»¶")

    if args.verbose and "performance_summary" in result:
        perf = result["performance_summary"]
        print(f"âš¡ å‡¦ç†é€Ÿåº¦: {perf.get('files_per_second', 'N/A')} ãƒ•ã‚¡ã‚¤ãƒ«/ç§’")

    # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ææ¡ˆ
    if high_issues > 0:
        print("\nğŸ¯ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
        print(f"  1. é«˜å„ªå…ˆåº¦ã®{high_issues}ä»¶ã‹ã‚‰å¯¾å‡¦ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™")
        print(f"  2. ãƒ¬ãƒãƒ¼ãƒˆã‚’ç¢ºèª: {args.output}")
    elif medium_issues > 0:
        print(
            f"\nğŸ’¡ è‰¯ã„çŠ¶æ…‹ã§ã™! ä¸­å„ªå…ˆåº¦ã®{medium_issues}ä»¶ã‹ã‚‰æ”¹å–„ã™ã‚‹ã¨ã•ã‚‰ã«è‰¯ããªã‚Šã¾ã™"
        )
    else:
        print("\nğŸŒŸ ç´ æ™´ã‚‰ã—ã„! ã‚¯ãƒªãƒ¼ãƒ³ãªã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®ã‚ˆã†ã§ã™")


if __name__ == "__main__":
    main()
